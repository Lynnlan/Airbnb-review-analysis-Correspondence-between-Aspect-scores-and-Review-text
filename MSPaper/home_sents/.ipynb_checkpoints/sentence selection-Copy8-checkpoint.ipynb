{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "from numpy.random import randn\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk\n",
    "# use wordnet find the first level keywords\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Necessary Data: reviews\n",
    "reviews_df = pd.read_csv(\"./Data/reviews.csv\", encoding=\"utf-8\")\n",
    "reviews_df.columns = ['home_id', 'review_id', 'date', 'reviewer_id', 'reviewer_name', 'comments']\n",
    "reviews_df.dropna()\n",
    "reviews_df.head(2)\n",
    "\n",
    "sample1_df = pd.read_csv(\"./Data/selected_data_for_research.csv\", sep='\\t', encoding=\"utf-8\")\n",
    "sample1_df = sample1_df.drop(\"Unnamed: 0\", axis=1)\n",
    "sample1_df.head(2)\n",
    "# Merge the reviews and homes in the sample data.\n",
    "df1 = sample1_df[['home_id', 'scores_cleanliness', 'scores_location']]\n",
    "df2 = reviews_df[['home_id', 'review_id', 'comments']]\n",
    "sample1_rh_df = pd.merge(df1, df2, on=\"home_id\")\n",
    "sample1_rh_df.head(3)\n",
    "# sample1_rh_df.stack()[0].comments\n",
    "\n",
    "# sample2_rh_df is a copy of sample1_rh_df to \n",
    "# in case unexpected modification for original data.\n",
    "sample2_rh_df = sample1_rh_df\n",
    "print('*' * 40 + '\\nThere are:\\n' + '-' * 40)\n",
    "print(str(len(sample2_rh_df.groupby('home_id'))) + \" Airbnb homes in total.\\n\" + '-' * 40)\n",
    "print(str(len(sample2_rh_df)) + \" reviews in total.\\n\" + '-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******\n",
    "### Here, the logic of determine the aspect that a sentence talking about is:\n",
    "\n",
    "* `aspect_keywords_dic` is a dictionary contains aspects and their relevant keywords.\n",
    "\n",
    "* Firstly, I use the Word2Vec similarity algorithm to count the similarity score between two words.\n",
    "\n",
    "* For each sentence, the aspect-similarity-score will `+1` when one word in it has the word-similarity-score with any word in keyword list lager than `0.7`(this number can change based on needs). \n",
    "\n",
    "* I tried use the total scores of all vectors(word-in-sentence to word-in-keywords), however, the results are very strange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_keywords_dic = {\n",
    "    'location': ['region', 'locality', 'neck_of_the_woods', 'location', 'vicinity',\n",
    "                 'neighbourhood', 'neighborhood'],\n",
    "    'cleanliness': ['tidy_up', 'straighten_out', 'cleanliness', 'clean', \n",
    "                    'neaten', 'square_away', 'straighten', 'clean_house', 'make_clean', \n",
    "                    'tidy', 'houseclean', 'clean_up', 'scavenge',\n",
    "                   'soiled', 'unclean', 'colly', 'bemire', 'uncleanliness', 'soil', 'begrime',\n",
    "                    'grime', 'untidy', 'dirty']\n",
    "}\n",
    "\n",
    "def sim_sents(doc, aspect, sim):\n",
    "    sim_sents = []\n",
    "    aspect_keywords = nlp(' '.join(aspect_keywords_dic[aspect]))\n",
    "    \n",
    "    for which_sen in range(len(list(doc.sents))):\n",
    "        new_doc = list(doc.sents)[which_sen].text        \n",
    "        sen_keywords = nlp(new_doc)\n",
    "        \n",
    "        if compute_score(sen_keywords, aspect_keywords, sim) > 0:\n",
    "            sim_sents.append(new_doc)\n",
    "    \n",
    "    return sim_sents\n",
    "\n",
    "def compute_score(sen_keywords, aspect_keywords, sim):\n",
    "    count = 0\n",
    "    for token1 in sen_keywords:\n",
    "        for token2 in aspect_keywords:\n",
    "            if token1.similarity(token2) >= sim:\n",
    "                count += 1 \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asp_rlt_sents_of_homes(aspect, sample_df):\n",
    "    \n",
    "    score_dic = {}\n",
    "    score_name = 'scores_'+aspect\n",
    "    score_series = sample_df.groupby(['home_id'])['review_id'].count()\n",
    "    summary_df = pd.DataFrame(columns=['home_id','num_of_reviews', 'num_of_sents',\n",
    "                                       'aspect', 'num_of_sents_0.5', 'num_of_sents_0.6', \n",
    "                                       'num_of_sents_0.7', 'sents_0.5', 'sents_0.6', 'sents_0.7'])\n",
    "\n",
    "    for i in range(len(score_series)):\n",
    "        \n",
    "        print(\"home\" + str(i+1))\n",
    "        home_id = score_series.index[i]\n",
    "        num_of_reviews = score_series.values[i]\n",
    "        num_of_sents = count_sents(home_id)[0]\n",
    "        num_of_sents_05, sents_05 = home_sents_process(home_id, aspect, 0.5)\n",
    "        num_of_sents_06, sents_06 = home_sents_process(home_id, aspect, 0.6)\n",
    "        num_of_sents_07, sents_07 = home_sents_process(home_id, aspect, 0.7)\n",
    "\n",
    "        summary_df.loc[i] = [home_id, num_of_reviews, num_of_sents, aspect, num_of_sents_05, \n",
    "                             num_of_sents_06, num_of_sents_07, sents_05, sents_06, sents_07]\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "def count_sents(home_id):\n",
    "    # all comments together\n",
    "    comments_list = [x for x in list(sample2_rh_df[sample2_rh_df.home_id == home_id].comments) if isinstance(x, str)]\n",
    "    comments = ''.join(comments_list)\n",
    "    doc = nlp(comments)\n",
    "    \n",
    "    return len(list(doc.sents)), doc\n",
    "    \n",
    "def home_sents_process(home_id, aspect, sim):\n",
    "    doc = count_sents(home_id)[1]\n",
    "#     sents_score_dic = sents_sim_score(doc, aspect, sim)\n",
    "#     home_asp_sents = sents_score_dic[sents_score_dic['aspect_sim_score'] != 0].text.values\n",
    "    home_asp_sents = sim_sents(doc, aspect, sim)\n",
    "\n",
    "    return len(home_asp_sents), home_asp_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the result of homes with different cleanliness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1466\n",
      "home1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample2_rh_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b706821a8a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloc_score10_hr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./Data/loc_score10_df.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_score10_hr_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloc_score10_homes_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masp_rlt_sents_of_homes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc_score10_hr_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mloc_score10_homes_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Data/loc_hs_df_10.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloc_score10_homes_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2ec36381ce2a>\u001b[0m in \u001b[0;36masp_rlt_sents_of_homes\u001b[0;34m(aspect, sample_df)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mhome_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnum_of_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mnum_of_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnum_of_sents_05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents_05\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhome_sents_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnum_of_sents_06\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents_06\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhome_sents_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2ec36381ce2a>\u001b[0m in \u001b[0;36mcount_sents\u001b[0;34m(home_id)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# all comments together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mcomments_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample2_rh_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample2_rh_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhome_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhome_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample2_rh_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Data: hr means homes&reviews\n",
    "loc_score10_hr_df = pd.read_csv(\"./Data/loc_score10_df.csv\", sep='\\t', encoding=\"utf-8\").drop(\"Unnamed: 0\", axis=1)\n",
    "print(len(loc_score10_hr_df))\n",
    "loc_score10_homes_sents = asp_rlt_sents_of_homes('location', loc_score10_hr_df)\n",
    "loc_score10_homes_sents.to_csv('./Data/loc_hs_df_10.csv', sep='\\t', encoding='utf-8')\n",
    "loc_score10_homes_sents.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
